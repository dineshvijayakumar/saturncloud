{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Language Modeling - CHAR RNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FjMJKaBXi9ru"},"source":["### Load tensorflow"]},{"cell_type":"code","metadata":{"id":"NdRMc0S0qZlv"},"source":["import tensorflow as tf\n","import numpy as np\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHgnLm-yhKVR"},"source":["### Collect Data\n","<font size=\"2\">Download data from https://s3.amazonaws.com/text-datasets/nietzsche.txt</font>"]},{"cell_type":"code","metadata":{"id":"YWEn-FtKhKVS"},"source":["!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRrsxDeAkGAE"},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USE1lx0aqZl3"},"source":["book_text = open('nietzsche.txt', encoding='utf8').read() #reading the book as a string\n","print('Length of the book: ' , len(book_text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B50PRvK_JmfS"},"source":["#book_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZcO2q06yVDr"},"source":["print(book_text[10000:10050])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jT0ptJlHqZmA"},"source":["### Tokenize the data"]},{"cell_type":"code","metadata":{"id":"WqoU8rqQqZmB"},"source":["#Tokenize at character level\n","t = tf.keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n","\n","#Fit tokenizer on the book\n","t.fit_on_texts(book_text)\n","\n","#Vocablury size\n","vocab_size = len(t.word_index)\n","\n","print('Number of unique characters: ', vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ynkKjzWhKVg"},"source":["#Character Vocabluty\n","print(t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PaqHLXghKVi"},"source":["#Convert characters in the book to Numbers\n","book_num = t.texts_to_sequences(book_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wE2RlSVcWc7"},"source":["print(book_text[10000:10050])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nC6EoXkkcZgD"},"source":["print(book_num[10000:10050])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm0pgAzaIa72"},"source":["#Build a dictionary which can convert numbers into chars\n","int_to_char = dict((i,c) for c, i in t.word_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcw5MRB4rVVS"},"source":["print(int_to_char)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41I8S0HnhKVn"},"source":["int_to_char[15]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhTY6QPdqZmY"},"source":["### Prepare Input and Output Sequences"]},{"cell_type":"markdown","metadata":{"id":"1IGYYK6ZqZmd"},"source":["Input and output container\n","- Input data will have sequences with 40 characters\n","- Output data will have one character which comes after 40 characters in the input data"]},{"cell_type":"code","metadata":{"id":"XfgONYykqZme"},"source":["sequence_length = 100 #Length of input sequence\n","\n","#Empty list for input and output data\n","input_data = []  #Empty list for input data\n","output_data = [] #Empty list for output data\n","\n","#Populate input and output data\n","for i in range(0, len(book_num) - sequence_length):\n","    \n","    input_seq = book_num[i : i + sequence_length] #input sequence    \n","    output_seq = book_num[i + sequence_length] #Output sequence\n","    \n","    input_data.append(input_seq)\n","    output_data.append(output_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbMa8MC60lHo"},"source":["print('Total number of input arrays: ', len(input_data))\n","print('Total number of Output arrays: ', len(output_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQnDvZxSdaCi"},"source":["print(\"Input Data length: \",len(input_data[10]))\n","print(\"Output Data length: \",len(output_data[10]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnPMoBOVsImR"},"source":["print(input_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6D3kDzWsQCl"},"source":["print(output_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLLac8rUqZmp"},"source":["### One Hot encoding for Input and Output"]},{"cell_type":"code","metadata":{"id":"tfvxJwgSZuLs"},"source":["#Input data one hot encoding\n","input_data_one_hot = tf.keras.utils.to_categorical(input_data,num_classes=vocab_size+1)\n","\n","#Output data one hot encoding\n","output_data = tf.keras.utils.to_categorical(output_data,num_classes=vocab_size+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5GPNL9Yy6RSc"},"source":["600793*100*85*4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwgfUPjd8TzB"},"source":["32*100*85*4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9AVnZf0RfsK"},"source":["input_data_one_hot.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBiXvlabRnbD"},"source":["output_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"831Cc5dkqZm7"},"source":["### Build Model"]},{"cell_type":"code","metadata":{"id":"5FmHZZrnqZm8"},"source":["#Build a Sequential Model\n","tf.keras.backend.clear_session()\n","model = tf.keras.models.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLz0Hir0R9Bk"},"source":["#Use an LSTM with memory size equal to 256\n","model.add(tf.keras.layers.LSTM(256, input_shape=(sequence_length,vocab_size+1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYWRJ71z-J2g"},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z18OnyzeR9-i"},"source":["#Output layer\n","model.add(tf.keras.layers.Dense(vocab_size+1, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhSgQgkfR0Qw"},"source":["#Compile model\n","model.compile(optimizer='adam', loss='categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRqHoulR3ful"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wlmHfhSUqZnW"},"source":["#### Train Model"]},{"cell_type":"code","metadata":{"id":"HAVH-aqyqZna"},"source":["model.fit(input_data_one_hot, output_data, \n","          batch_size=128, \n","          epochs=500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2rPX39v_DBi"},"source":["600593//128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7M5pUAR6k7-"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCfKwezE6xc_"},"source":["save_path = '/gdrive/My Drive/Great Learning/Sequential NLP/Notebooks/4. Seq2Seq Model/char_rnn.h5'\n","model.save(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lNY7Tnb87P3I"},"source":["#### Generating Text"]},{"cell_type":"code","metadata":{"id":"ATTnzbmD-6VQ"},"source":["save_path = '/gdrive/My Drive/Great Learning/Sequential NLP/Notebooks/4. Seq2Seq Model/char_rnn.h5'\n","model = tf.keras.models.load_model(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljXhywqfgOg6"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JApAG7DdhQ9J"},"source":["#1 - 'My name' -> ' ' \n","#2 - 'y name ' -> 'i'\n","#3 - ' name i' -> 's'\n","#4 - 'name is' -> ' '"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaKc7ulE7SWt"},"source":["def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype(\"float64\")\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmlFY9Kq7j56"},"source":["def predict_seq_with_sample(test_seq, num_chars=50, tempreture=1.0):\n","    \n","    #Initialize predicted output\n","    predicted_output = ''\n","    \n","    #lets predict 50 next chars\n","    current_seq = np.copy(test_seq)\n","    \n","    for i in tqdm(range(num_chars)):\n","\n","        #One hot encoding\n","        current_seq_one_hot = tf.keras.utils.to_categorical(current_seq, num_classes=vocab_size+1)\n","        \n","        #Convert it into a batch of 1 example\n","        data_input = np.reshape(current_seq_one_hot,(1,\n","                                                     current_seq_one_hot.shape[0],\n","                                                     current_seq_one_hot.shape[1]))\n","        \n","        #Take sample prediction\n","        preds = model.predict(data_input)[0]\n","        predicted_char_int = sample(preds, temperature=tempreture)\n","        #Get the char int with maximum probability\n","        #predicted_char_int = np.argmax(model.predict(data_input)[0])\n","        \n","        if (predicted_char_int != 0):\n","            \n","            #Add to the predicted out, convert int to char\n","            predicted_output = predicted_output + int_to_char[predicted_char_int]\n","        \n","        #Update seq with new value at the end\n","        current_seq = np.roll(current_seq, -1)\n","        current_seq[current_seq.shape[0]-1] = [predicted_char_int]\n","    \n","    print('')\n","    print('')\n","\n","    print('Initial sequence is: ')\n","    for i in range(len(test_seq)):\n","        print(int_to_char[test_seq[i][0]], end='')\n","    \n","    print('')\n","    print('')\n","    print('Generated sequence is: ')\n","    print(predicted_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ff9fOiv98TBa"},"source":["#Identify a random sequence which we will use to generate output\n","start_pos = np.random.randint(0, high=(len(book_num) - sequence_length))\n","test_seq =  book_num[start_pos : start_pos+sequence_length]\n","\n","predict_seq_with_sample(test_seq, num_chars=500, tempreture=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpkOpQ-mVp4l"},"source":["#### Word2Vec Embedding Model for Char-RNN"]},{"cell_type":"code","metadata":{"id":"93ydZLs2VupO"},"source":["#Build a Sequential Model\n","model_wv = tf.keras.models.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ufGNgrw4V1o9"},"source":["model_wv.add(tf.keras.layers.Embedding(85, #Number of unique chars\n","                                       10, #Embedding Size\n","                                       input_length=40\n","                                       ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"suElZr5CWyNO"},"source":["model_wv.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d97PvArTXA6B"},"source":["#Add LSTM\n","model_wv.add(tf.keras.layers.LSTM(256, activation='relu'))\n","\n","#Add output layer\n","model_wv.add(tf.keras.layers.Dense(85, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPux3IHuXcpY"},"source":["model_wv.compile(optimizer='adam', loss='categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMS6ND-3Xm-r"},"source":["model_wv.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIGTbNdNXxdt"},"source":["np.array(input_data).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5o1NTZxXuUl"},"source":["model_wv.fit(np.array(input_data), output_data,\n","             batch_size=128,\n","             epochs=1)"],"execution_count":null,"outputs":[]}]}